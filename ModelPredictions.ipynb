{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch_loader\n",
    "reload(torch_loader)\n",
    "reload(preprocessing)\n",
    "import preprocessing as pre\n",
    "\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pickle...\n",
      "Done!\n",
      "For lag: 365 nan percent is 0.153\n",
      "For lag: 91 nan percent is 0.038\n",
      "Done loading and preprocessing set!\n",
      "0\n",
      "{'tseries': \n",
      "\n",
      "Columns 0 to 9 \n",
      " 5.2311  4.7791  4.9273  5.5872  5.4072  5.0173  5.0106  5.1985  4.9628  5.2781\n",
      "\n",
      "Columns 10 to 19 \n",
      " 5.4161  5.7961  5.4161  5.2417  4.9488  5.9738  5.9558  6.3154  6.3852  6.0234\n",
      "\n",
      "Columns 20 to 29 \n",
      " 5.7398  5.7900  5.5334  5.5491  5.6490  5.7104  5.3033  5.2149  5.2257  4.9698\n",
      "\n",
      "Columns 30 to 39 \n",
      " 4.7185  5.1358  5.1818  4.7449  4.8040  5.2883  5.2679  5.2311  5.7301  5.5683\n",
      "\n",
      "Columns 40 to 49 \n",
      " 5.2470  5.2311  5.1059  4.9053  4.8978  4.9836  5.0039  4.9273  4.3307  4.6728\n",
      "\n",
      "Columns 50 to 59 \n",
      " 4.4067  4.8675  5.2149  5.2883  4.9200  4.6250  4.6540  4.4427  4.4886  4.7449\n",
      "\n",
      "Columns 60 to 69 \n",
      " 4.9488  4.5539  4.1897  4.0254  4.2485  4.4308  4.7005  4.5951  4.2341  4.0073\n",
      "\n",
      "Columns 70 to 79 \n",
      " 3.8067  4.4886  4.5109  4.5109  4.5109  3.6889  3.9703  3.9120  3.7377  3.9703\n",
      "\n",
      "Columns 80 to 89 \n",
      " 4.6634  4.6728  4.1431  4.1897  4.1897  3.7842  4.1589  4.7185  4.7875  4.6540\n",
      "\n",
      "Columns 90 to 99 \n",
      " 4.2341  4.0775  4.2485  4.3307  4.9628  4.6821  4.0775  3.9890  3.8918  3.7612\n",
      "\n",
      "Columns 100 to 109 \n",
      " 4.0073  4.7449  4.5109  4.0254  3.6376  3.9890  3.8286  3.9120  4.4188  4.3694\n",
      "\n",
      "Columns 110 to 119 \n",
      " 4.0073  3.9512  3.9512  3.8712  3.8918  4.4188  4.4998  3.7377  3.9318  4.0254\n",
      "\n",
      "Columns 120 to 129 \n",
      " 3.7842  3.7136  4.2905  4.2047  3.5835  3.8501  3.7377  3.9318  4.0073  4.2905\n",
      "\n",
      "Columns 130 to 139 \n",
      " 4.5109  3.8712  3.9318  3.8067  3.9318  3.8067  4.7005  4.3820  4.1109  3.7377\n",
      "\n",
      "Columns 140 to 149 \n",
      " 4.1589  3.7136  3.8501  4.3438  4.6052  4.0254  4.0431  5.0106  4.8520  4.5326\n",
      "\n",
      "Columns 150 to 159 \n",
      " 4.7875  4.5433  4.0943  4.0775  4.0943  4.3307  4.0775  4.5326  4.4998  4.1431\n",
      "\n",
      "Columns 160 to 169 \n",
      " 3.9318  4.0431  4.2047  3.9120  4.3307  4.2905  3.9120  3.8067  3.9318  3.4340\n",
      "\n",
      "Columns 170 to 179 \n",
      " 4.3175  4.4998  4.1589  4.2047  3.2958  4.0604  3.7377  3.9512  4.4998  4.2195\n",
      "\n",
      "Columns 180 to 189 \n",
      " 4.1744  3.9318  4.0254  3.5835  4.6151  4.3307  4.5539  4.0073  3.8501  3.9120\n",
      "\n",
      "Columns 190 to 199 \n",
      " 4.0073  4.0073  4.0431  4.1589  3.7842  3.6109  3.8918  3.5264  3.6109  4.1744\n",
      "\n",
      "Columns 200 to 209 \n",
      " 4.2767  4.0604  4.0254  4.3041  4.2767  4.4067  4.5326  3.9890  3.8286  3.8501\n",
      "\n",
      "Columns 210 to 219 \n",
      " 4.1271  4.0073  4.3307  4.4188  4.4886  4.1109  4.0604  4.1271  4.2195  4.4427\n",
      "\n",
      "Columns 220 to 229 \n",
      " 4.8442  5.5491  5.9216  5.5683  5.3706  5.9054  5.7494  6.1048  5.8289  5.2933\n",
      "\n",
      "Columns 230 to 239 \n",
      " 5.1120  4.9767  5.0562  4.8978  5.2095  5.1240  4.7005  4.6444  4.5643  4.4886\n",
      "\n",
      "Columns 240 to 249 \n",
      " 4.3307  4.9972  4.8520  4.7622  4.6151  4.4188  4.6347  4.5326  4.3175  4.5747\n",
      "\n",
      "Columns 250 to 259 \n",
      " 4.5539  4.2905  4.3041  4.7791  5.6021  5.5947  5.6419  5.0752  5.1761  4.9836\n",
      "\n",
      "Columns 260 to 269 \n",
      " 4.8828  4.7536  5.3799  5.1705  4.8828  4.5433  4.4188  5.2095  5.8319  5.9026\n",
      "\n",
      "Columns 270 to 279 \n",
      " 5.9965  5.5683  5.2883  5.3375  5.1930  5.2832  5.7137  5.8230  5.5797  5.5255\n",
      "\n",
      "Columns 280 to 289 \n",
      " 5.0434  4.8598  4.8040  5.1648  5.2204  4.5326  4.7449  5.6768  5.2364  4.9628\n",
      "\n",
      "Columns 290 to 299 \n",
      " 5.1180  4.9558  4.8903  4.9273  4.8978  4.9767  4.8598  5.2832  5.4250  5.1120\n",
      "\n",
      "Columns 300 to 309 \n",
      " 4.9200  6.2442  6.4394  6.6134  7.0157  7.1123  6.9660  6.6294  6.4457  6.2558\n",
      "\n",
      "Columns 310 to 319 \n",
      " 6.1654  6.9866  6.6921  6.1485  6.0981  5.8608  5.6419  5.6595  6.1654  6.2086\n",
      "\n",
      "Columns 320 to 329 \n",
      " 5.7038  5.5835  5.5683  5.4681  5.5413  6.0307  5.8889  5.4638  5.3891  5.2204\n",
      "\n",
      "Columns 330 to 339 \n",
      " 5.4972  5.3471  5.5530  5.4337  4.9273  5.0106  4.7707  4.7875  4.6444  4.9273\n",
      "\n",
      "Columns 340 to 349 \n",
      " 5.1358  4.7274  4.6250  4.4427  4.8520  5.0499  5.1985  5.0039  4.4543  4.6540\n",
      "\n",
      "Columns 350 to 359 \n",
      " 4.4886  4.4998  4.4998  4.4308  4.7622  4.2195  4.0431  4.0604  3.9703  4.0073\n",
      "\n",
      "Columns 360 to 369 \n",
      " 4.4188  4.1589  4.0254  4.2485  4.2047  3.7842  3.8067  4.6347  4.7707  4.1109\n",
      "\n",
      "Columns 370 to 379 \n",
      " 4.2767  4.1271  4.1744  4.3041  4.2485  4.3820  4.0775  4.1589  3.7377  3.6636\n",
      "\n",
      "Columns 380 to 389 \n",
      " 3.5835  3.8286  3.7377  4.0604  3.5835  4.0431  3.8501  3.7136  4.5433  4.8978\n",
      "\n",
      "Columns 390 to 399 \n",
      " 4.2195  4.2341  3.9120  4.0604  3.9512  4.6052  4.7185  4.2341  4.2485  4.3820\n",
      "\n",
      "Columns 400 to 409 \n",
      " 4.3567  4.6634  4.9558  4.8040  4.3438  4.4427  4.1431  4.1109  4.0775  4.2195\n",
      "\n",
      "Columns 410 to 419 \n",
      " 4.4188  4.1744  3.7612  4.1744  4.3944  4.2767  4.7185  4.5109  4.0604  4.0431\n",
      "\n",
      "Columns 420 to 429 \n",
      " 4.2485  3.8712  4.4659  4.4773  4.1431  3.9318  4.0431  3.4965  3.8501  3.8712\n",
      "\n",
      "Columns 430 to 439 \n",
      " 4.2767  4.0775  3.8286  3.8501  3.8286  3.4340  3.7842  3.8918  3.6109  3.4340\n",
      "\n",
      "Columns 440 to 449 \n",
      " 3.5835  3.7612  3.9703  4.1589  4.2047  4.1271  3.7377  3.6636  3.6889  3.6109\n",
      "\n",
      "Columns 450 to 459 \n",
      " 3.7377  4.2627  4.3041  3.5553  4.3041  4.2905  3.9890  3.8712  4.1271  3.9890\n",
      "\n",
      "Columns 460 to 469 \n",
      " 3.5835  3.8286  3.6889  3.8712  4.4427  4.7185  4.4308  4.1589  4.3694  4.1589\n",
      "\n",
      "Columns 470 to 479 \n",
      " 4.3820  4.2047  4.7274  4.4886  4.1589  4.0775  4.1109  3.9890  4.5326  4.7536\n",
      "\n",
      "Columns 480 to 489 \n",
      " 4.8122  4.0943  4.6634  4.8903  5.2523  5.1475  5.7004  5.8833  5.3279  5.1358\n",
      "\n",
      "Columns 490 to 499 \n",
      " 4.7875  4.7449  4.7185  5.2679  5.2983  4.7622  4.6347  4.3944  4.5326  4.3567\n",
      "\n",
      "Columns 500 to 509 \n",
      " 5.0938  4.9273  4.5643  3.9512  4.3041  3.9890  4.0604  4.3694  4.2767  3.6109\n",
      "\n",
      "Columns 510 to 519 \n",
      " 4.0254  3.8501  3.4965  3.2189  3.9703  3.8918  3.5264  3.8918  3.7377  3.4340\n",
      "\n",
      "Columns 520 to 529 \n",
      " 3.6376  3.8918  3.9890  3.8501  3.4965  3.7136  3.4012  3.4965  4.1589  3.7842\n",
      "\n",
      "Columns 530 to 539 \n",
      " 3.6636  3.8918  3.1355  3.0445  4.1271  3.8501  3.6376  3.3673  3.2189  3.6376\n",
      "\n",
      "Columns 540 to 549 \n",
      " 3.3322  3.2958  3.3322  3.8501  3.3673  3.2958  3.2958  2.7726  3.6889  3.4012\n",
      "[torch.DoubleTensor of size 1x550]\n",
      ", 'agent': \n",
      "-0.7071 -0.7071  1.4142\n",
      "[torch.DoubleTensor of size 1x3]\n",
      ", 'country': \n",
      "-0.3780 -0.3780 -0.3780 -0.3780 -0.3780 -0.3780 -0.3780  2.6458\n",
      "[torch.DoubleTensor of size 1x8]\n",
      ", 'site': \n",
      "-0.7071  1.4142 -0.7071\n",
      "[torch.DoubleTensor of size 1x3]\n",
      ", 'median': \n",
      "1.00000e-02 *\n",
      "  2.8483\n",
      "[torch.DoubleTensor of size 1]\n",
      ", 'quarterly': \n",
      " 0.2726\n",
      "[torch.DoubleTensor of size 1]\n",
      ", 'yearly': \n",
      "1.00000e-02 *\n",
      "  6.5860\n",
      "[torch.DoubleTensor of size 1]\n",
      ", 'dow': \n",
      "\n",
      "Columns 0 to 9 \n",
      "-0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011\n",
      "\n",
      "Columns 10 to 19 \n",
      " 1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001\n",
      "\n",
      "Columns 20 to 29 \n",
      "-0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008\n",
      "\n",
      "Columns 30 to 39 \n",
      " 0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017\n",
      "\n",
      "Columns 40 to 49 \n",
      "-1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995\n",
      "\n",
      "Columns 50 to 59 \n",
      " 0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014\n",
      "\n",
      "Columns 60 to 69 \n",
      " 1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998\n",
      "\n",
      "Columns 70 to 79 \n",
      "-0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011\n",
      "\n",
      "Columns 80 to 89 \n",
      " 1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001\n",
      "\n",
      "Columns 90 to 99 \n",
      "-0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008\n",
      "\n",
      "Columns 100 to 109 \n",
      " 0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017\n",
      "\n",
      "Columns 110 to 119 \n",
      "-1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995\n",
      "\n",
      "Columns 120 to 129 \n",
      " 0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014\n",
      "\n",
      "Columns 130 to 139 \n",
      " 1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998\n",
      "\n",
      "Columns 140 to 149 \n",
      "-0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011\n",
      "\n",
      "Columns 150 to 159 \n",
      " 1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001\n",
      "\n",
      "Columns 160 to 169 \n",
      "-0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008\n",
      "\n",
      "Columns 170 to 179 \n",
      " 0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017\n",
      "\n",
      "Columns 180 to 189 \n",
      "-1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995\n",
      "\n",
      "Columns 190 to 199 \n",
      " 0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014\n",
      "\n",
      "Columns 200 to 209 \n",
      " 1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998\n",
      "\n",
      "Columns 210 to 219 \n",
      "-0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011\n",
      "\n",
      "Columns 220 to 229 \n",
      " 1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001\n",
      "\n",
      "Columns 230 to 239 \n",
      "-0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008\n",
      "\n",
      "Columns 240 to 249 \n",
      " 0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017\n",
      "\n",
      "Columns 250 to 259 \n",
      "-1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995\n",
      "\n",
      "Columns 260 to 269 \n",
      " 0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014\n",
      "\n",
      "Columns 270 to 279 \n",
      " 1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998\n",
      "\n",
      "Columns 280 to 289 \n",
      "-0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011\n",
      "\n",
      "Columns 290 to 299 \n",
      " 1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001\n",
      "\n",
      "Columns 300 to 309 \n",
      "-0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008\n",
      "\n",
      "Columns 310 to 319 \n",
      " 0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017\n",
      "\n",
      "Columns 320 to 329 \n",
      "-1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995\n",
      "\n",
      "Columns 330 to 339 \n",
      " 0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014\n",
      "\n",
      "Columns 340 to 349 \n",
      " 1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998\n",
      "\n",
      "Columns 350 to 359 \n",
      "-0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011\n",
      "\n",
      "Columns 360 to 369 \n",
      " 1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001\n",
      "\n",
      "Columns 370 to 379 \n",
      "-0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008\n",
      "\n",
      "Columns 380 to 389 \n",
      " 0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017\n",
      "\n",
      "Columns 390 to 399 \n",
      "-1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995\n",
      "\n",
      "Columns 400 to 409 \n",
      " 0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014\n",
      "\n",
      "Columns 410 to 419 \n",
      " 1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998\n",
      "\n",
      "Columns 420 to 429 \n",
      "-0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011\n",
      "\n",
      "Columns 430 to 439 \n",
      " 1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001\n",
      "\n",
      "Columns 440 to 449 \n",
      "-0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008\n",
      "\n",
      "Columns 450 to 459 \n",
      " 0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017\n",
      "\n",
      "Columns 460 to 469 \n",
      "-1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995\n",
      "\n",
      "Columns 470 to 479 \n",
      " 0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014\n",
      "\n",
      "Columns 480 to 489 \n",
      " 1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998\n",
      "\n",
      "Columns 490 to 499 \n",
      "-0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011\n",
      "\n",
      "Columns 500 to 509 \n",
      " 1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001\n",
      "\n",
      "Columns 510 to 519 \n",
      "-0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008\n",
      "\n",
      "Columns 520 to 529 \n",
      " 0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017\n",
      "\n",
      "Columns 530 to 539 \n",
      "-1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995\n",
      "\n",
      "Columns 540 to 549 \n",
      " 0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014\n",
      "\n",
      "Columns 550 to 559 \n",
      " 1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998\n",
      "\n",
      "Columns 560 to 569 \n",
      "-0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011\n",
      "\n",
      "Columns 570 to 579 \n",
      " 1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001\n",
      "\n",
      "Columns 580 to 589 \n",
      "-0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008\n",
      "\n",
      "Columns 590 to 599 \n",
      " 0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017\n",
      "\n",
      "Columns 600 to 609 \n",
      "-1.5001 -0.9998 -0.4995  0.0008  0.5011  1.0014  1.5017 -1.5001 -0.9998 -0.4995\n",
      "[torch.DoubleTensor of size 1x610]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# default prediction window = 60, sliding_window=30\n",
    "ds = torch_loader.WebTrafficDataset('data', 'train_1')\n",
    "\n",
    "\n",
    "train_loader = DataLoader(ds,\n",
    "                         batch_size=1,\n",
    "                         shuffle=True,\n",
    "                         num_workers=4)\n",
    "\n",
    "for batch_idx, data in enumerate(train_loader):\n",
    "    print(batch_idx)\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(345,)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(res, tar) = pre.series_supervised_np(ds[0]['tseries'], lags=10, drop_nan=True)\n",
    "\n",
    "tar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential (\n",
       "  (gru1): GRUCell(1, 30)\n",
       "  (gru2): GRUCell(30, 30)\n",
       "  (linear): Linear (30 -> 1)\n",
       ")"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Sequential(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_size, bias=True):\n",
    "        super(Sequential, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.bias = bias\n",
    "        \n",
    "        self.gru1 = nn.GRUCell(self.input_size, self.hidden_size, bias=self.bias)\n",
    "        self.gru2 = nn.GRUCell(self.hidden_size, self.hidden_size, bias=self.bias)\n",
    "        self.linear = nn.Linear(self.hidden_size, self.input_size)\n",
    "        \n",
    "    def forward(self, input, future=0):\n",
    "        hidden_t1 = Variable(torch.zeros(input.size(0), self.hidden_size).double(),\n",
    "                            requires_grad=False)\n",
    "        hidden_t2 = Variable(torch.zeros(input.size(0), self.hidden_size).double(),\n",
    "                            requires_grad=False)\n",
    "        \n",
    "        outputs = []\n",
    "        for i, input_t in enumerate(input.chunk(input.size(0), dim=1)):\n",
    "            # input should be (seq_len, batch_size, input_size) for LSTM\n",
    "            # (batch, input_size) for GRUCell\n",
    "            hidden_t1 = self.gru1(input_t, hidden_t1)\n",
    "            hidden_t2 = self.gru2(hidden_t1, hidden_t2)\n",
    "            output = self.linear(hidden_t2)\n",
    "            outputs += [output]\n",
    "            \n",
    "        for i in range(future):\n",
    "            hidden_t1 = self.gru1(output, hidden_t1)\n",
    "            hidden_t2 = self.gru2(hidden_t1, hidden_t2)\n",
    "            output = self.linear(hidden_t2)\n",
    "            outputs += [output]\n",
    "            \n",
    "        outputs = torch.stack(outputs, 1).squeeze(2)\n",
    "        out = outputs[-1]\n",
    "        return out\n",
    "input_size = 1 # number of features. 1 for just tseries\n",
    "hidden_size = 30\n",
    "batch_size = 30\n",
    "\n",
    "model = Sequential(input_size, hidden_size, batch_size)\n",
    "model.double()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM (\n",
       "  (rnn): LSTM(1, 30, dropout=0.2)\n",
       "  (fc1): Linear (30 -> 30)\n",
       "  (bn1): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (fc2): Linear (30 -> 1)\n",
       "  (relu): ReLU ()\n",
       ")"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.2):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=self.input_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=1,\n",
    "            dropout=dropout,\n",
    "            bidirectional=False)\n",
    "        self.fc1 = nn.Linear(self.hidden_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(self.hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        x = x.view(seq_len, batch_size, -1)\n",
    "        \n",
    "        h0 = Variable(torch.zeros(seq_len, batch_size, self.hidden_size).double())\n",
    "        c0 = Variable(torch.zeros(seq_len, batch_size, self.hidden_size).double())\n",
    "        \n",
    "        out, (ht, ct) = self.rnn(x, (h0, c0))\n",
    "        \n",
    "        out = out[-1]\n",
    "        out = self.fc1(out)\n",
    "        out = self.bn1(out)\n",
    "        #out = self.relu(out)\n",
    "        out = F.dropout(out, training=self.training)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "input_size = 1 # number of features. 1 for just tseries\n",
    "hidden_size = 30\n",
    "\n",
    "model = LSTM(input_size, hidden_size)\n",
    "model.double()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.3039\n",
       "-0.1010\n",
       " 0.4717\n",
       " 0.2621\n",
       "-0.0829\n",
       " 0.1347\n",
       "-0.0653\n",
       " 0.3231\n",
       " 0.3139\n",
       "-0.0099\n",
       " 1.1192\n",
       " 0.4854\n",
       " 0.1369\n",
       " 1.1210\n",
       "-0.0098\n",
       " 0.1521\n",
       " 0.2832\n",
       " 0.2568\n",
       " 0.0024\n",
       "-0.2461\n",
       " 0.1698\n",
       "-0.0019\n",
       " 0.5822\n",
       "-0.2708\n",
       " 0.2001\n",
       " 0.3001\n",
       " 0.4438\n",
       "-0.0468\n",
       "-0.1867\n",
       "-0.2638\n",
       "-0.3405\n",
       " 0.3832\n",
       " 0.1530\n",
       " 0.1851\n",
       " 0.4477\n",
       " 0.1686\n",
       " 0.0416\n",
       "-0.2460\n",
       "-0.1712\n",
       " 0.0494\n",
       " 0.1101\n",
       " 0.3905\n",
       " 0.3892\n",
       " 0.4086\n",
       " 0.1440\n",
       "-0.0900\n",
       "-0.0505\n",
       "-0.3584\n",
       " 0.4311\n",
       " 0.0006\n",
       " 0.4333\n",
       "-0.0501\n",
       "-0.0887\n",
       " 0.0893\n",
       " 0.0638\n",
       "-0.0988\n",
       "-0.1819\n",
       " 0.0572\n",
       " 0.1163\n",
       " 0.0274\n",
       " 0.4229\n",
       "-0.2425\n",
       " 0.0163\n",
       "-0.0130\n",
       "-0.1448\n",
       "-0.1074\n",
       " 0.1139\n",
       " 0.2515\n",
       " 0.2237\n",
       " 0.0692\n",
       " 0.3213\n",
       " 0.7680\n",
       " 0.1913\n",
       "-0.1075\n",
       "-0.3192\n",
       " 0.2462\n",
       " 0.2421\n",
       " 0.2896\n",
       " 0.1369\n",
       "-0.3404\n",
       " 0.3318\n",
       "-0.1796\n",
       "-0.0613\n",
       "-0.3879\n",
       " 0.2594\n",
       " 0.3578\n",
       " 0.2507\n",
       " 0.2042\n",
       " 0.1083\n",
       "-0.2094\n",
       " 0.1002\n",
       "-0.1328\n",
       "-0.1301\n",
       " 0.2676\n",
       "-0.0762\n",
       " 0.2070\n",
       "-0.0556\n",
       "-0.2748\n",
       " 0.0145\n",
       "-0.0426\n",
       "-0.1525\n",
       "-0.0551\n",
       " 0.2095\n",
       " 0.4746\n",
       " 0.2810\n",
       "-0.0132\n",
       "-0.2295\n",
       "-0.2211\n",
       "-0.0529\n",
       "-0.1315\n",
       "-0.0722\n",
       " 0.1816\n",
       " 0.0675\n",
       "-0.0859\n",
       " 0.0371\n",
       "-0.2208\n",
       "-0.0615\n",
       " 0.0484\n",
       "-0.5368\n",
       "-0.6026\n",
       " 0.3281\n",
       " 0.3508\n",
       " 0.1845\n",
       " 0.4908\n",
       "-0.3745\n",
       "-0.1665\n",
       " 0.0326\n",
       "-0.3653\n",
       "-0.2851\n",
       "-0.2020\n",
       " 0.0003\n",
       " 0.2353\n",
       " 0.3533\n",
       "-0.2949\n",
       "-0.0446\n",
       "-0.0300\n",
       " 0.0956\n",
       "-0.3346\n",
       "-0.2669\n",
       "-0.1113\n",
       " 0.1293\n",
       " 0.0943\n",
       "-0.1357\n",
       "-0.0308\n",
       " 0.0216\n",
       "-0.5642\n",
       "-0.3597\n",
       "-0.3354\n",
       "-0.0952\n",
       "-1.1027\n",
       "-0.1747\n",
       "-0.1833\n",
       " 0.0228\n",
       "-0.0625\n",
       "-0.2890\n",
       "-0.5746\n",
       "-0.3379\n",
       "-0.0177\n",
       "-0.5539\n",
       "-0.1472\n",
       "-0.1624\n",
       " 0.0357\n",
       "-0.0690\n",
       "-0.0786\n",
       "-0.1739\n",
       "-0.2432\n",
       "-0.1412\n",
       " 0.0756\n",
       "-0.0770\n",
       "-0.4090\n",
       "-0.1368\n",
       " 0.0215\n",
       "-0.4368\n",
       "-0.3927\n",
       "-0.2481\n",
       "-0.0947\n",
       "-0.3286\n",
       "-0.1573\n",
       "-0.1828\n",
       "-0.3186\n",
       " 0.0550\n",
       "-0.0642\n",
       "-0.9167\n",
       "-0.0678\n",
       "-0.0532\n",
       "-0.5203\n",
       "-0.1459\n",
       "-0.2518\n",
       "-0.5689\n",
       "-0.2301\n",
       "-0.4939\n",
       "-0.3495\n",
       "-0.2593\n",
       "-0.1088\n",
       "-1.2001\n",
       "-0.1963\n",
       "-0.1403\n",
       "-0.1020\n",
       "-0.2158\n",
       "-0.0629\n",
       "-0.5266\n",
       "-0.4005\n",
       "-0.0347\n",
       "-0.5550\n",
       "-0.0773\n",
       "-0.4211\n",
       "-0.3785\n",
       "-0.1506\n",
       "-0.0308\n",
       "-0.0271\n",
       "-0.0839\n",
       "-0.1507\n",
       "-0.5324\n",
       "-0.1639\n",
       "-0.1709\n",
       "-0.5124\n",
       "-0.2520\n",
       "-0.0551\n",
       "-0.0939\n",
       "-0.2532\n",
       "-0.0845\n",
       " 0.0289\n",
       "-0.1234\n",
       "-0.1805\n",
       "-0.4037\n",
       "-0.2324\n",
       " 0.0140\n",
       "-0.1797\n",
       "-0.8770\n",
       "-0.4720\n",
       "-0.2887\n",
       "-0.0857\n",
       "-0.3006\n",
       "-0.4397\n",
       "-0.1080\n",
       "-0.0207\n",
       "-0.1406\n",
       "-0.5619\n",
       "-0.6409\n",
       "-0.2181\n",
       "-0.1803\n",
       "-0.3893\n",
       " 0.0774\n",
       "-0.3033\n",
       "-0.0861\n",
       "-0.1775\n",
       "-0.9930\n",
       "-0.5694\n",
       "-0.7283\n",
       "-0.5205\n",
       "-0.0681\n",
       "-0.2928\n",
       "-0.0334\n",
       "-0.1063\n",
       " 0.0224\n",
       "-0.8163\n",
       "-0.3221\n",
       " 0.2185\n",
       "-0.1423\n",
       " 1.8651\n",
       "-0.2445\n",
       "-0.2050\n",
       "-0.1677\n",
       "-0.1512\n",
       "-0.2908\n",
       " 0.2038\n",
       " 0.0577\n",
       "-0.3091\n",
       "-0.0788\n",
       " 0.0387\n",
       " 0.0343\n",
       "-0.1000\n",
       "-0.1689\n",
       "-0.4311\n",
       "-0.4330\n",
       "-0.3211\n",
       "-0.3753\n",
       " 2.1341\n",
       " 0.4819\n",
       " 0.0451\n",
       "-0.1590\n",
       "-0.0714\n",
       "-0.8642\n",
       "-0.7216\n",
       "-0.3625\n",
       "-0.0890\n",
       " 2.3426\n",
       " 0.2331\n",
       " 0.2712\n",
       "-0.0925\n",
       " 0.0358\n",
       " 0.0139\n",
       " 0.0019\n",
       "-0.9440\n",
       "-0.1473\n",
       " 0.1838\n",
       " 0.0321\n",
       " 0.0053\n",
       " 0.5306\n",
       " 0.4050\n",
       "-0.6619\n",
       "-0.4364\n",
       "-0.1525\n",
       " 0.0270\n",
       " 1.4166\n",
       " 0.3067\n",
       " 0.2577\n",
       " 0.3403\n",
       " 0.4735\n",
       "-0.4079\n",
       " 0.0853\n",
       "-0.5080\n",
       "-0.5661\n",
       " 0.3932\n",
       " 0.3264\n",
       "-0.1166\n",
       " 0.5197\n",
       " 0.1746\n",
       "-0.1284\n",
       "-0.7285\n",
       "-0.3067\n",
       "-0.1321\n",
       " 0.5100\n",
       " 1.0237\n",
       "-0.1589\n",
       " 0.5404\n",
       " 0.1581\n",
       "-0.1302\n",
       "-0.6372\n",
       " 0.0601\n",
       "-0.5009\n",
       " 0.9280\n",
       " 0.0501\n",
       " 0.1605\n",
       " 0.5135\n",
       "-0.2530\n",
       "-0.2385\n",
       "-0.0944\n",
       "-0.1568\n",
       "-0.1566\n",
       " 0.1907\n",
       " 0.7100\n",
       "-0.0331\n",
       "-0.1642\n",
       "-0.3886\n",
       "-0.2262\n",
       "-0.7360\n",
       "-0.1402\n",
       "-0.0748\n",
       "-0.2251\n",
       "-0.1612\n",
       " 0.1410\n",
       " 0.3054\n",
       " 0.9486\n",
       "-0.1390\n",
       "-0.5962\n",
       "-0.2558\n",
       "-0.0320\n",
       "-0.2826\n",
       "-0.2010\n",
       " 0.0451\n",
       " 0.7322\n",
       " 0.9794\n",
       "-0.4844\n",
       "-0.3058\n",
       "-0.1900\n",
       "-0.0906\n",
       "-0.2041\n",
       "-0.8961\n",
       " 0.1138\n",
       " 0.1982\n",
       " 0.2937\n",
       "-0.0659\n",
       "-0.3411\n",
       "-0.2859\n",
       "-0.0035\n",
       "-0.6423\n",
       "-0.4583\n",
       "-0.0250\n",
       "-0.0471\n",
       "-0.0248\n",
       "-0.2882\n",
       "-0.3834\n",
       "-0.1308\n",
       "-0.1138\n",
       "-0.3331\n",
       "-0.2097\n",
       " 0.0339\n",
       "-0.1347\n",
       "-0.3803\n",
       "-0.3148\n",
       "-0.8068\n",
       " 0.0246\n",
       "-0.2036\n",
       "-0.4239\n",
       "-0.4402\n",
       " 0.0313\n",
       "-0.0224\n",
       "-0.2415\n",
       "-0.3826\n",
       "-0.2460\n",
       "-0.2790\n",
       "-0.0530\n",
       "-0.7313\n",
       " 0.1180\n",
       "-0.0012\n",
       "-0.0453\n",
       "-0.6344\n",
       "-0.0676\n",
       "-0.1390\n",
       "-0.2026\n",
       "-0.0960\n",
       "-0.4353\n",
       "-0.3272\n",
       " 0.0114\n",
       " 0.0867\n",
       "-0.2019\n",
       "-0.2602\n",
       "-0.2536\n",
       "-0.1342\n",
       "-0.0080\n",
       "-0.2746\n",
       "-0.5477\n",
       "-0.0139\n",
       "-0.0637\n",
       "-0.0305\n",
       "-0.2758\n",
       "-0.2590\n",
       "-0.0421\n",
       "-0.1987\n",
       "-0.3555\n",
       "-0.1681\n",
       " 0.0471\n",
       "-0.0628\n",
       "-0.4379\n",
       "-0.1003\n",
       "-0.1925\n",
       "-0.0352\n",
       "-0.1634\n",
       "-0.0952\n",
       "-0.4405\n",
       " 0.2051\n",
       " 0.0317\n",
       " 0.0161\n",
       "-0.4407\n",
       "-0.2967\n",
       "-0.1075\n",
       "-0.1834\n",
       "-0.1437\n",
       " 0.0391\n",
       " 0.0564\n",
       "-0.0759\n",
       "-0.1550\n",
       "-0.1149\n",
       "-0.4384\n",
       " 0.1028\n",
       "-0.2276\n",
       "-0.0766\n",
       "-0.1420\n",
       " 0.1361\n",
       " 0.0455\n",
       " 0.0191\n",
       "-0.1437\n",
       "-0.0534\n",
       "-0.1171\n",
       "-0.4241\n",
       "-0.0772\n",
       "-0.1752\n",
       "-0.3688\n",
       " 0.4069\n",
       "-0.6044\n",
       "-0.1456\n",
       "-0.1981\n",
       " 0.0231\n",
       "-0.3688\n",
       "-0.0275\n",
       "-0.1033\n",
       " 0.2121\n",
       " 1.3648\n",
       " 0.1247\n",
       "-0.1631\n",
       "-0.0925\n",
       "-0.0527\n",
       " 0.0140\n",
       "-0.0407\n",
       "-0.1249\n",
       "-0.0789\n",
       " 0.8358\n",
       "-0.3600\n",
       " 0.6183\n",
       "-0.2288\n",
       "-0.2108\n",
       "-0.2256\n",
       "-0.2266\n",
       "-0.1000\n",
       " 0.0875\n",
       " 0.6617\n",
       " 0.0558\n",
       " 0.5747\n",
       " 0.3717\n",
       "-0.0574\n",
       "-0.2331\n",
       "-0.2871\n",
       "-0.0916\n",
       " 0.1278\n",
       " 1.1439\n",
       " 0.0029\n",
       " 0.5594\n",
       " 0.4230\n",
       " 0.1286\n",
       "-0.0096\n",
       "-0.2064\n",
       "-0.1502\n",
       " 0.5698\n",
       " 0.4010\n",
       " 0.1379\n",
       " 1.1145\n",
       " 0.5830\n",
       "-0.0595\n",
       " 0.0319\n",
       "-0.1532\n",
       "-0.0530\n",
       "-0.1195\n",
       " 1.3031\n",
       "-0.2866\n",
       " 0.3519\n",
       " 0.0755\n",
       " 0.1875\n",
       " 0.6054\n",
       " 0.7154\n",
       "-0.0319\n",
       " 0.2856\n",
       " 0.6321\n",
       " 0.1065\n",
       "-0.0825\n",
       " 0.1307\n",
       " 0.3730\n",
       "-0.1326\n",
       " 0.6282\n",
       " 0.2343\n",
       "[torch.DoubleTensor of size 540x1]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tseries = (ds[10000]['tseries'].reshape(-1))\n",
    "(tseries, target) = pre.series_supervised_np(tseries, lags=10, drop_nan=True)\n",
    "tseries = Variable(torch.from_numpy(tseries))\n",
    "out = model(tseries)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/145063 (0%)]\tLoss: 1.844445\n",
      "Train Epoch: 1 [10/145063 (0%)]\tLoss: 12.832309\n",
      "Train Epoch: 1 [20/145063 (0%)]\tLoss: 1.017194\n",
      "Train Epoch: 1 [30/145063 (0%)]\tLoss: 13.301526\n",
      "Train Epoch: 1 [40/145063 (0%)]\tLoss: 0.769080\n",
      "Train Epoch: 1 [50/145063 (0%)]\tLoss: 9.796189\n",
      "Train Epoch: 1 [60/145063 (0%)]\tLoss: 1.587932\n",
      "Train Epoch: 1 [70/145063 (0%)]\tLoss: 0.846916\n",
      "Train Epoch: 1 [80/145063 (0%)]\tLoss: 7.098526\n",
      "Train Epoch: 1 [90/145063 (0%)]\tLoss: 0.723116\n",
      "Train Epoch: 1 [100/145063 (0%)]\tLoss: 7.783759\n",
      "Train Epoch: 1 [110/145063 (0%)]\tLoss: 1.550737\n",
      "Train Epoch: 1 [120/145063 (0%)]\tLoss: 6.247850\n",
      "Train Epoch: 1 [130/145063 (0%)]\tLoss: 6.351440\n",
      "Train Epoch: 1 [140/145063 (0%)]\tLoss: 4.385904\n",
      "Train Epoch: 1 [150/145063 (0%)]\tLoss: 6.422185\n",
      "Train Epoch: 1 [160/145063 (0%)]\tLoss: 10.031444\n",
      "Train Epoch: 1 [170/145063 (0%)]\tLoss: 9.267537\n",
      "Train Epoch: 1 [180/145063 (0%)]\tLoss: 2.392793\n",
      "Train Epoch: 1 [190/145063 (0%)]\tLoss: 2.063300\n",
      "Train Epoch: 1 [200/145063 (0%)]\tLoss: 1.402950\n",
      "Train Epoch: 1 [210/145063 (0%)]\tLoss: 0.726502\n",
      "Train Epoch: 1 [220/145063 (0%)]\tLoss: 1.067729\n",
      "Train Epoch: 1 [230/145063 (0%)]\tLoss: 17.012792\n",
      "Train Epoch: 1 [240/145063 (0%)]\tLoss: 0.894591\n",
      "Train Epoch: 1 [250/145063 (0%)]\tLoss: 2.509636\n",
      "Train Epoch: 1 [260/145063 (0%)]\tLoss: 16.537160\n",
      "Train Epoch: 1 [270/145063 (0%)]\tLoss: 1.419044\n",
      "Train Epoch: 1 [280/145063 (0%)]\tLoss: 16.356042\n",
      "Train Epoch: 1 [290/145063 (0%)]\tLoss: 5.444885\n",
      "Train Epoch: 1 [300/145063 (0%)]\tLoss: 3.068111\n",
      "Train Epoch: 1 [310/145063 (0%)]\tLoss: 2.109038\n",
      "Train Epoch: 1 [320/145063 (0%)]\tLoss: 10.843882\n",
      "Train Epoch: 1 [330/145063 (0%)]\tLoss: 16.360369\n",
      "Train Epoch: 1 [340/145063 (0%)]\tLoss: 13.659081\n",
      "Train Epoch: 1 [350/145063 (0%)]\tLoss: 17.760741\n",
      "Train Epoch: 1 [360/145063 (0%)]\tLoss: 16.343316\n",
      "Train Epoch: 1 [370/145063 (0%)]\tLoss: 7.647983\n",
      "Train Epoch: 1 [380/145063 (0%)]\tLoss: 5.137842\n",
      "Train Epoch: 1 [390/145063 (0%)]\tLoss: 5.037631\n",
      "Train Epoch: 1 [400/145063 (0%)]\tLoss: 2.992889\n",
      "Train Epoch: 1 [410/145063 (0%)]\tLoss: 16.736239\n",
      "Train Epoch: 1 [420/145063 (0%)]\tLoss: 11.033727\n",
      "Train Epoch: 1 [430/145063 (0%)]\tLoss: 1.370559\n",
      "Train Epoch: 1 [440/145063 (0%)]\tLoss: 5.562202\n",
      "Train Epoch: 1 [450/145063 (0%)]\tLoss: 14.133945\n",
      "Train Epoch: 1 [460/145063 (0%)]\tLoss: 5.861733\n",
      "Train Epoch: 1 [470/145063 (0%)]\tLoss: 9.225827\n",
      "Train Epoch: 1 [480/145063 (0%)]\tLoss: 2.078689\n",
      "Train Epoch: 1 [490/145063 (0%)]\tLoss: 4.134234\n",
      "Train Epoch: 1 [500/145063 (0%)]\tLoss: 1.460504\n",
      "Train Epoch: 1 [510/145063 (0%)]\tLoss: 16.313876\n",
      "Train Epoch: 1 [520/145063 (0%)]\tLoss: 2.532579\n",
      "Train Epoch: 1 [530/145063 (0%)]\tLoss: 1.297423\n",
      "Train Epoch: 1 [540/145063 (0%)]\tLoss: 6.082549\n",
      "Train Epoch: 1 [550/145063 (0%)]\tLoss: 1.604120\n",
      "Train Epoch: 1 [560/145063 (0%)]\tLoss: 0.725839\n",
      "Train Epoch: 1 [570/145063 (0%)]\tLoss: 20.302341\n",
      "Train Epoch: 1 [580/145063 (0%)]\tLoss: 12.873181\n",
      "Train Epoch: 1 [590/145063 (0%)]\tLoss: 3.852774\n",
      "Train Epoch: 1 [600/145063 (0%)]\tLoss: 6.011227\n",
      "Train Epoch: 1 [610/145063 (0%)]\tLoss: 15.248921\n",
      "Train Epoch: 1 [620/145063 (0%)]\tLoss: 16.715787\n",
      "Train Epoch: 1 [630/145063 (0%)]\tLoss: 1.184526\n",
      "Train Epoch: 1 [640/145063 (0%)]\tLoss: 4.781639\n",
      "Train Epoch: 1 [650/145063 (0%)]\tLoss: 1.585160\n",
      "Train Epoch: 1 [660/145063 (0%)]\tLoss: 3.066019\n",
      "Train Epoch: 1 [670/145063 (0%)]\tLoss: 8.087623\n",
      "Train Epoch: 1 [680/145063 (0%)]\tLoss: 3.298023\n",
      "Train Epoch: 1 [690/145063 (0%)]\tLoss: 1.068811\n",
      "Train Epoch: 1 [700/145063 (0%)]\tLoss: 11.785936\n",
      "Train Epoch: 1 [710/145063 (0%)]\tLoss: 3.066116\n",
      "Train Epoch: 1 [720/145063 (0%)]\tLoss: 16.512403\n",
      "Train Epoch: 1 [730/145063 (1%)]\tLoss: 11.218398\n",
      "Train Epoch: 1 [740/145063 (1%)]\tLoss: 10.991798\n",
      "Train Epoch: 1 [750/145063 (1%)]\tLoss: 4.752226\n",
      "Train Epoch: 1 [760/145063 (1%)]\tLoss: 5.726351\n",
      "Train Epoch: 1 [770/145063 (1%)]\tLoss: 4.178490\n",
      "Train Epoch: 1 [780/145063 (1%)]\tLoss: 3.300630\n",
      "Train Epoch: 1 [790/145063 (1%)]\tLoss: 20.650583\n",
      "Train Epoch: 1 [800/145063 (1%)]\tLoss: 6.050122\n",
      "Train Epoch: 1 [810/145063 (1%)]\tLoss: 3.776505\n",
      "Train Epoch: 1 [820/145063 (1%)]\tLoss: 11.956565\n",
      "Train Epoch: 1 [830/145063 (1%)]\tLoss: 1.088311\n",
      "Train Epoch: 1 [840/145063 (1%)]\tLoss: 11.104249\n",
      "Train Epoch: 1 [850/145063 (1%)]\tLoss: 5.358380\n",
      "Train Epoch: 1 [860/145063 (1%)]\tLoss: 12.887235\n",
      "Train Epoch: 1 [870/145063 (1%)]\tLoss: 1.701614\n",
      "Train Epoch: 1 [880/145063 (1%)]\tLoss: 6.314684\n",
      "Train Epoch: 1 [890/145063 (1%)]\tLoss: 8.725185\n",
      "Train Epoch: 1 [900/145063 (1%)]\tLoss: 2.666481\n",
      "Train Epoch: 1 [910/145063 (1%)]\tLoss: 22.132485\n",
      "Train Epoch: 1 [920/145063 (1%)]\tLoss: 14.714950\n",
      "Train Epoch: 1 [930/145063 (1%)]\tLoss: 1.567300\n",
      "Train Epoch: 1 [940/145063 (1%)]\tLoss: 7.272701\n",
      "Train Epoch: 1 [950/145063 (1%)]\tLoss: 2.457626\n",
      "Train Epoch: 1 [960/145063 (1%)]\tLoss: 3.670971\n",
      "Train Epoch: 1 [970/145063 (1%)]\tLoss: 1.685819\n",
      "Train Epoch: 1 [980/145063 (1%)]\tLoss: 1.533596\n",
      "Train Epoch: 1 [990/145063 (1%)]\tLoss: 6.700936\n",
      "Train Epoch: 1 [1000/145063 (1%)]\tLoss: 1.121272\n",
      "Train Epoch: 1 [1010/145063 (1%)]\tLoss: 2.385100\n",
      "Train Epoch: 1 [1020/145063 (1%)]\tLoss: 8.874096\n",
      "Train Epoch: 1 [1030/145063 (1%)]\tLoss: 10.072594\n",
      "Train Epoch: 1 [1040/145063 (1%)]\tLoss: 1.053290\n",
      "Train Epoch: 1 [1050/145063 (1%)]\tLoss: 8.856743\n",
      "Train Epoch: 1 [1060/145063 (1%)]\tLoss: 12.978713\n",
      "Train Epoch: 1 [1070/145063 (1%)]\tLoss: 2.836182\n",
      "Train Epoch: 1 [1080/145063 (1%)]\tLoss: 8.899040\n",
      "Train Epoch: 1 [1090/145063 (1%)]\tLoss: 8.460046\n",
      "Train Epoch: 1 [1100/145063 (1%)]\tLoss: 4.802653\n",
      "Train Epoch: 1 [1110/145063 (1%)]\tLoss: 3.802856\n",
      "Train Epoch: 1 [1120/145063 (1%)]\tLoss: 3.049159\n",
      "Train Epoch: 1 [1130/145063 (1%)]\tLoss: 2.597045\n",
      "Train Epoch: 1 [1140/145063 (1%)]\tLoss: 4.338391\n",
      "Train Epoch: 1 [1150/145063 (1%)]\tLoss: 3.788104\n",
      "Train Epoch: 1 [1160/145063 (1%)]\tLoss: 1.730160\n",
      "Train Epoch: 1 [1170/145063 (1%)]\tLoss: 6.800214\n",
      "Train Epoch: 1 [1180/145063 (1%)]\tLoss: 2.312679\n",
      "Train Epoch: 1 [1190/145063 (1%)]\tLoss: 11.725082\n",
      "Train Epoch: 1 [1200/145063 (1%)]\tLoss: 2.028996\n",
      "Train Epoch: 1 [1210/145063 (1%)]\tLoss: 3.785686\n",
      "Train Epoch: 1 [1220/145063 (1%)]\tLoss: 15.639118\n",
      "Train Epoch: 1 [1230/145063 (1%)]\tLoss: 2.842270\n",
      "Train Epoch: 1 [1240/145063 (1%)]\tLoss: 5.794564\n",
      "Train Epoch: 1 [1250/145063 (1%)]\tLoss: 5.166289\n",
      "Train Epoch: 1 [1260/145063 (1%)]\tLoss: 1.973433\n",
      "Train Epoch: 1 [1270/145063 (1%)]\tLoss: 12.313159\n",
      "Train Epoch: 1 [1280/145063 (1%)]\tLoss: 2.818859\n",
      "Train Epoch: 1 [1290/145063 (1%)]\tLoss: 8.836744\n",
      "Train Epoch: 1 [1300/145063 (1%)]\tLoss: 9.053623\n",
      "Train Epoch: 1 [1310/145063 (1%)]\tLoss: 6.997599\n",
      "Train Epoch: 1 [1320/145063 (1%)]\tLoss: 4.275218\n",
      "Train Epoch: 1 [1330/145063 (1%)]\tLoss: 6.361959\n",
      "Train Epoch: 1 [1340/145063 (1%)]\tLoss: 1.283426\n",
      "Train Epoch: 1 [1350/145063 (1%)]\tLoss: 14.943796\n",
      "Train Epoch: 1 [1360/145063 (1%)]\tLoss: 1.220281\n",
      "Train Epoch: 1 [1370/145063 (1%)]\tLoss: 4.434007\n",
      "Train Epoch: 1 [1380/145063 (1%)]\tLoss: 9.568318\n",
      "Train Epoch: 1 [1390/145063 (1%)]\tLoss: 1.425863\n",
      "Train Epoch: 1 [1400/145063 (1%)]\tLoss: 1.701542\n",
      "Train Epoch: 1 [1410/145063 (1%)]\tLoss: 3.311799\n",
      "Train Epoch: 1 [1420/145063 (1%)]\tLoss: 4.815817\n",
      "Train Epoch: 1 [1430/145063 (1%)]\tLoss: 4.318435\n",
      "Train Epoch: 1 [1440/145063 (1%)]\tLoss: 1.470300\n",
      "Train Epoch: 1 [1450/145063 (1%)]\tLoss: 9.120342\n",
      "Train Epoch: 1 [1460/145063 (1%)]\tLoss: 11.714052\n",
      "Train Epoch: 1 [1470/145063 (1%)]\tLoss: 4.145398\n",
      "Train Epoch: 1 [1480/145063 (1%)]\tLoss: 7.985178\n",
      "Train Epoch: 1 [1490/145063 (1%)]\tLoss: 5.892301\n",
      "Train Epoch: 1 [1500/145063 (1%)]\tLoss: 1.945657\n",
      "Train Epoch: 1 [1510/145063 (1%)]\tLoss: 10.713286\n",
      "Train Epoch: 1 [1520/145063 (1%)]\tLoss: 3.456398\n",
      "Train Epoch: 1 [1530/145063 (1%)]\tLoss: 4.689456\n",
      "Train Epoch: 1 [1540/145063 (1%)]\tLoss: 4.110522\n",
      "Train Epoch: 1 [1550/145063 (1%)]\tLoss: 1.378480\n",
      "Train Epoch: 1 [1560/145063 (1%)]\tLoss: 2.246625\n",
      "Train Epoch: 1 [1570/145063 (1%)]\tLoss: 2.886565\n",
      "Train Epoch: 1 [1580/145063 (1%)]\tLoss: 17.075143\n",
      "Train Epoch: 1 [1590/145063 (1%)]\tLoss: 9.084980\n",
      "Train Epoch: 1 [1600/145063 (1%)]\tLoss: 5.098609\n",
      "Train Epoch: 1 [1610/145063 (1%)]\tLoss: 2.865300\n",
      "Train Epoch: 1 [1620/145063 (1%)]\tLoss: 17.709571\n",
      "Train Epoch: 1 [1630/145063 (1%)]\tLoss: 3.382134\n",
      "Train Epoch: 1 [1640/145063 (1%)]\tLoss: 4.805910\n",
      "Train Epoch: 1 [1650/145063 (1%)]\tLoss: 3.356063\n",
      "Train Epoch: 1 [1660/145063 (1%)]\tLoss: 8.754677\n",
      "Train Epoch: 1 [1670/145063 (1%)]\tLoss: 5.684270\n",
      "Train Epoch: 1 [1680/145063 (1%)]\tLoss: 8.587288\n",
      "Train Epoch: 1 [1690/145063 (1%)]\tLoss: 9.181790\n",
      "Train Epoch: 1 [1700/145063 (1%)]\tLoss: 20.061133\n",
      "Train Epoch: 1 [1710/145063 (1%)]\tLoss: 24.899020\n",
      "Train Epoch: 1 [1720/145063 (1%)]\tLoss: 1.944179\n",
      "Train Epoch: 1 [1730/145063 (1%)]\tLoss: 2.114966\n",
      "Train Epoch: 1 [1740/145063 (1%)]\tLoss: 2.172058\n",
      "Train Epoch: 1 [1750/145063 (1%)]\tLoss: 4.961022\n",
      "Train Epoch: 1 [1760/145063 (1%)]\tLoss: 2.046116\n",
      "Train Epoch: 1 [1770/145063 (1%)]\tLoss: 7.617735\n",
      "Train Epoch: 1 [1780/145063 (1%)]\tLoss: 3.850599\n",
      "Train Epoch: 1 [1790/145063 (1%)]\tLoss: 3.610876\n",
      "Train Epoch: 1 [1800/145063 (1%)]\tLoss: 13.417472\n",
      "Train Epoch: 1 [1810/145063 (1%)]\tLoss: 6.354854\n",
      "Train Epoch: 1 [1820/145063 (1%)]\tLoss: 8.589343\n",
      "Train Epoch: 1 [1830/145063 (1%)]\tLoss: 13.342560\n",
      "Train Epoch: 1 [1840/145063 (1%)]\tLoss: 1.496739\n",
      "Train Epoch: 1 [1850/145063 (1%)]\tLoss: 11.426602\n",
      "Train Epoch: 1 [1860/145063 (1%)]\tLoss: 2.727109\n",
      "Train Epoch: 1 [1870/145063 (1%)]\tLoss: 12.617220\n",
      "Train Epoch: 1 [1880/145063 (1%)]\tLoss: 1.067082\n",
      "Train Epoch: 1 [1890/145063 (1%)]\tLoss: 2.318217\n",
      "Train Epoch: 1 [1900/145063 (1%)]\tLoss: 3.963492\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=1e-3, \n",
    "                           weight_decay=1e-3, momentum=0.2)\n",
    "    \n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        tseries = data['tseries'].numpy().reshape(-1)\n",
    "        (tseries, target) = pre.series_supervised_np(np.nan_to_num(tseries), \n",
    "                                                     lags=10, drop_nan=True)\n",
    "        \n",
    "        input = Variable(torch.from_numpy(tseries))\n",
    "        target = Variable(torch.from_numpy(target))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(input)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        losses.append(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx, len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "        if batch_idx == 10000:\n",
    "            break\n",
    "\n",
    "train(1)\n",
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cross validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
